# FEDERATED LEARNING FOR SMART HEALTHCARE

From the paper: [Federated-Learning-for-Smart-Healthcare-A Survey.pdf](/Paper/Federated-Learning-for-Smart-Healthcare-A%20Survey.pdf)

## What is FL?

Federated Learning (FL) is a distributed artificial intelligence (AI) approach that enables the training of high-quality AI models by **averaging local updates aggregated from multiple clients**, such as hospitals or Internet of Medical Things (IoMT) devices, without needing direct access to the raw data. This approach is particularly attractive for smart healthcare because it can coordinate multiple clients to perform AI training without sharing sensitive raw data

---

## Features of FL

Federated Learning (FL) has several key features that make it attractive for smart healthcare, especially in scenarios where data privacy is a concern. Here's a breakdown of these features:

- Data Privacy Improvement: FL allows local health data to remain at local medical sites and devices, with only local updates, such as model gradients, being required by the central server for AI training. This reduces the risk of sensitive user information being leaked to external third parties, thus providing a higher degree of user privacy.

- Trade-off between Accuracy and Utility: FL offers a balance between accuracy, utility, and privacy. While it retains model generalizability, it may result in a nominal accuracy loss compared to conventional centralized learning.

- Low-Cost Health Data Training: FL can significantly reduce communication costs, such as latency and transmit power, by avoiding the transfer of large data volumes to the server. Model gradients are generally smaller in size compared to actual datasets, which saves network bandwidth and reduces the possibility of network congestion in large healthcare networks.

- Three main categories of FL: Horizontal Federated Learning (HFL), Vertical Federated Learning (VFL), and Federated Transfer Learning (FTL).

Despite these benefits, it is important to note that FL does not fully address the privacy problem in smart healthcare and dedicated privacy protection mechanisms may be needed to enhance FL in healthcare networks

## How does it work?

The federated learning (FL) process for smart healthcare involves several key steps:

1. **System Initialization and Client Selection**: The aggregation server selects a healthcare analytic task, like medical imaging or human motion detection. It also determines model requirements, such as task classification or prediction, and learning parameters like neural node numbers and learning rates. A subset of clients (e.g., hospitals) is chosen to participate in the FL process.

2. **Distributed Local Training and Updates**: The server sends an initial model, including a global gradient, to the selected clients, triggering distributed training. In each communication round, each client trains a local model using its own dataset and calculates its model update, such as the gradient in neural networks. The client then uploads its model update to the server.

3. **Model Aggregation and Download**: The server aggregates the updates received from the selected clients using an aggregation method such as the Federated Averaging (FedAvg) algorithm. In FedAvg, the gradient parameters of local models are averaged element-wise with weights proportional to the sizes of the client datasets. The server then calculates a new version of the global model and broadcasts it to all clients as the basis for further local model updates in the next learning round. The FL process iterates until the global loss function converges

## How many types of FL?

In the context of federated learning (FL) for smart healthcare, there are **three main types of FL** described: **Horizontal FL (HFL)**, **Vertical FL (VFL)**, and **Federated Transfer Learning (FTL)**.

### 1. Horizontal Federated Learning (HFL)
Horizontal FL (HFL) is used when healthcare clients train a shared global model using datasets with the **same feature space** but **different sample spaces**. The key characteristics of HFL include:

- **Local Model Training:** Local FL participants use the same AI model to train their respective datasets.
- **Global Model Aggregation:** A central server combines local updates from participants to create a global update, without requiring direct access to local data.
- **Example:** Detecting speech disorders where multiple users speak the same sentence (feature space) with different voices (sample space) on their smartphones. The local updates are aggregated by a parameter server to enhance the global model.

### 2. Vertical Federated Learning (VFL)
Vertical FL (VFL) applies to scenarios where health datasets share the **same sample space** but have **different feature spaces**. The main features of VFL include:

- **Entity Alignment:** Solutions based on entity alignment address overlapping data samples at distributed clients. Encryption techniques are often integrated during local training to ensure data security.
- **Example:** A shared learning model among entities like hospitals and insurance companies. These entities serve the same patients (sample space) but manage different types of data:
  - Hospitals: Medical records.
  - Insurance companies: Healthcare costs.

### 3. Federated Transfer Learning (FTL)
Federated Transfer Learning (FTL) is designed to handle datasets with **different sample spaces** and **different feature spaces**. FTL leverages transfer learning techniques to:

- Calculate feature values from different feature spaces into a unified representation.
- Train local datasets based on the transformed feature representation.

By leveraging these types of FL, smart healthcare systems can enhance collaboration across distributed data sources while maintaining data privacy and security.

---

## What are some FL designs?

The sources discuss several advanced Federated Learning (FL) designs tailored to smart healthcare applications. These designs address challenges such as resource constraints, security vulnerabilities, privacy concerns, and the need for personalized services. Below is a breakdown of the key FL designs:

### 1. Resource-Aware FL
This design optimizes the use of computing and communication resources in FL systems, particularly in healthcare settings with resource-limited devices, such as IoMT (Internet of Medical Things) devices.

- **Device Scheduling:** Selects a subset of IoMT devices for training to minimize total training time.
- **Resource Allocation:** Jointly optimizes device scheduling and resource allocation to enhance FL performance, including bandwidth allocation among clients using the same or different FL services.
- **Multi-Armed Bandit (MAB) Theory:** Addresses challenges caused by unknown channel state information between IoMT devices and the aggregation server.
- **Hierarchical Federated Edge Learning:** Aggregation is partially performed by intermediary nodes, such as hospitals, to reduce communication overhead.

### 2. Secure FL
This design mitigates security vulnerabilities in FL systems, such as poisoning attacks, inference attacks, and malicious servers, ensuring secure and trustworthy learning.

- **Reputation-Based Device Selection:** Introduces a reputation system to select reliable devices and prevent updates from untrusted devices.
- **Decentralized FL:** Removes reliance on a central server using methods like gossip, consensus, and diffusion. Blockchain can be used to manage user reputation and coordinate global model calculations in a peer-to-peer manner.
- **Secure Aggregation:** Protects the aggregation process at the central server using methods like:
  - Secure communication protocols.
  - Dining Cryptographers (DC)-based secure aggregation.
  - Homomorphic threshold encryption.
  - Pairwise masking.

### 3. Privacy-Enhanced FL
This design addresses privacy concerns, such as membership attacks, unintentional data leakage, and inference attacks.

- **Differential Privacy:** Adds artificial noise to local datasets or model updates to ensure privacy. This requires balancing privacy guarantees with model accuracy.
- **Bandwidth-Efficient FL:** Reduces data transmission by:
  - Sending only the signs of updated values.
  - Using techniques like sparse ternary compression.
- **FL-SIGN-DP:** Ensures that hospitals cannot infer the global model broadcast by the aggregation server.

### 4. Incentive-Aware FL
This design encourages participation in FL by addressing resource limitations, privacy concerns, and trust issues that might discourage IoMT devices from sharing their models.

- **Game Theory:** Uses tools like the Stackelberg game to incentivize user participation by maximizing the utility of both IoMT devices and the aggregation server.
- **Deep Reinforcement Learning (DRL):** Helps:
  - Aggregation servers determine reward strategies.
  - Devices decide on local training schemes.
- **Data Contribution Evaluation:** Evaluates device contributions based on metrics like:
  - Data quantity.
  - Data quality.
  - Device reputation.

### 5. Personalized FL
This design creates personalized models for individual users rather than a single global model, recognizing the unique physical characteristics, environments, and needs of each user.

- **Edge-Cloud Architectures:** Combines local model training at the network edge (e.g., at home) with global model aggregation in the cloud.
- **Label Heterogeneity:** Addresses challenges where each device defines labels differently. Techniques like ùõº-weighted updates are used to aggregate overlapping label information.

### 6. Combining FL Designs
These FL designs are not mutually exclusive and can be combined to create robust and effective systems. For example:
- A healthcare application might use resource-aware techniques, secure aggregation, and differential privacy simultaneously to address multiple challenges.
- Specific design choices depend on the requirements and constraints of the healthcare application.

By adopting these advanced FL designs, smart healthcare systems can achieve efficient, secure, and personalized solutions while maintaining data privacy and addressing resource limitations.

---

## Keyword
Artificial Intelligence - Federated Learning - Smart healthcare

## Glossary
# Glossary (Thu·∫≠t ng·ªØ)

| **Term** | **Definition** | **Translation (Ti·∫øng Vi·ªát)** |
|----------|--------------|---------------------------|
| **Federated Learning (FL)** | A distributed AI approach that enables training AI models without sharing raw data, by aggregating local updates from multiple clients. | H·ªçc li√™n k·∫øt (FL) l√† m·ªôt ph∆∞∆°ng ph√°p AI ph√¢n t√°n cho ph√©p hu·∫•n luy·ªán m√¥ h√¨nh AI m√† kh√¥ng c·∫ßn chia s·∫ª d·ªØ li·ªáu g·ªëc, b·∫±ng c√°ch t·ªïng h·ª£p c√°c b·∫£n c·∫≠p nh·∫≠t c·ª•c b·ªô t·ª´ nhi·ªÅu thi·∫øt b·ªã kh√°c nhau. |
| **Horizontal Federated Learning (HFL)** | FL type where clients share the same feature space but have different sample spaces. | H·ªçc li√™n k·∫øt ngang (HFL) l√† m·ªôt lo·∫°i FL trong ƒë√≥ c√°c kh√°ch h√†ng c√≥ c√πng kh√¥ng gian ƒë·∫∑c tr∆∞ng nh∆∞ng kh√°c kh√¥ng gian m·∫´u. |
| **Vertical Federated Learning (VFL)** | FL type where clients share the same sample space but have different feature spaces. | H·ªçc li√™n k·∫øt d·ªçc (VFL) l√† m·ªôt lo·∫°i FL trong ƒë√≥ c√°c kh√°ch h√†ng c√≥ c√πng kh√¥ng gian m·∫´u nh∆∞ng kh√°c kh√¥ng gian ƒë·∫∑c tr∆∞ng. |
| **Federated Transfer Learning (FTL)** | FL type where clients have both different sample spaces and different feature spaces, leveraging transfer learning techniques. | H·ªçc li√™n k·∫øt chuy·ªÉn giao (FTL) l√† m·ªôt lo·∫°i FL trong ƒë√≥ c√°c kh√°ch h√†ng c√≥ c·∫£ kh√¥ng gian m·∫´u v√† kh√¥ng gian ƒë·∫∑c tr∆∞ng kh√°c nhau, t·∫≠n d·ª•ng k·ªπ thu·∫≠t h·ªçc chuy·ªÉn giao. |
| **Federated Averaging (FedAvg)** | A method for aggregating local updates in FL by averaging model gradients proportionally to dataset sizes. | Ph∆∞∆°ng ph√°p trung b√¨nh li√™n k·∫øt (FedAvg) l√† m·ªôt c√°ch t·ªïng h·ª£p c√°c b·∫£n c·∫≠p nh·∫≠t c·ª•c b·ªô trong FL b·∫±ng c√°ch l·∫•y trung b√¨nh c√°c gradient m√¥ h√¨nh theo k√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu. |
| **Internet of Medical Things (IoMT)** | A network of medical devices that collect and share health-related data. | Internet v·∫°n v·∫≠t y t·∫ø (IoMT) l√† m·∫°ng l∆∞·ªõi c√°c thi·∫øt b·ªã y t·∫ø thu th·∫≠p v√† chia s·∫ª d·ªØ li·ªáu s·ª©c kh·ªèe. |
| **Differential Privacy** | A privacy-enhancing technique that adds noise to data to prevent individual data identification. | B·∫£o m·∫≠t vi sai l√† m·ªôt k·ªπ thu·∫≠t b·∫£o v·ªá quy·ªÅn ri√™ng t∆∞ b·∫±ng c√°ch th√™m nhi·ªÖu v√†o d·ªØ li·ªáu ƒë·ªÉ ngƒÉn ch·∫∑n vi·ªác nh·∫≠n d·∫°ng c√° nh√¢n. |
| **Secure Aggregation** | A method in FL that ensures updates from clients remain private during the aggregation process. | T·ªïng h·ª£p b·∫£o m·∫≠t l√† ph∆∞∆°ng ph√°p trong FL gi√∫p b·∫£o v·ªá t√≠nh ri√™ng t∆∞ c·ªßa c√°c b·∫£n c·∫≠p nh·∫≠t t·ª´ kh√°ch h√†ng trong qu√° tr√¨nh t·ªïng h·ª£p m√¥ h√¨nh. |
| **Personalized FL** | An FL approach that creates tailored AI models for individual users instead of a single global model. | H·ªçc li√™n k·∫øt c√° nh√¢n h√≥a l√† ph∆∞∆°ng ph√°p FL t·∫°o ra m√¥ h√¨nh AI ph√π h·ª£p v·ªõi t·ª´ng ng∆∞·ªùi d√πng thay v√¨ m·ªôt m√¥ h√¨nh chung. |
| **Reputation-Based Device Selection** | A technique in FL that selects trustworthy clients based on their past performance and reputation. | Ch·ªçn thi·∫øt b·ªã d·ª±a tr√™n danh ti·∫øng l√† k·ªπ thu·∫≠t trong FL d√πng ƒë·ªÉ ch·ªçn c√°c thi·∫øt b·ªã ƒë√°ng tin c·∫≠y d·ª±a tr√™n hi·ªáu su·∫•t tr∆∞·ªõc ƒë√≥ v√† danh ti·∫øng c·ªßa ch√∫ng. |
| **Decentralized FL** | A federated learning method that eliminates reliance on a central server, often using blockchain. | H·ªçc li√™n k·∫øt phi t·∫≠p trung l√† ph∆∞∆°ng ph√°p FL kh√¥ng ph·ª• thu·ªôc v√†o m√°y ch·ªß trung t√¢m, th∆∞·ªùng s·ª≠ d·ª•ng blockchain. |
| **Multi-Armed Bandit (MAB) Theory** | A decision-making algorithm used in FL to optimize resource allocation under uncertainty. | L√Ω thuy·∫øt Multi-Armed Bandit (MAB) l√† thu·∫≠t to√°n ra quy·∫øt ƒë·ªãnh ƒë∆∞·ª£c s·ª≠ d·ª•ng trong FL ƒë·ªÉ t·ªëi ∆∞u h√≥a ph√¢n b·ªï t√†i nguy√™n trong ƒëi·ªÅu ki·ªán kh√¥ng ch·∫Øc ch·∫Øn. |
| **Game Theory in FL** | Used to design incentive mechanisms for encouraging client participation in FL. | L√Ω thuy·∫øt tr√≤ ch∆°i trong FL ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ thi·∫øt k·∫ø c√°c c∆° ch·∫ø khuy·∫øn kh√≠ch s·ª± tham gia c·ªßa kh√°ch h√†ng trong FL. |
| **Deep Reinforcement Learning (DRL)** | A machine learning technique used in FL to improve training strategies and resource allocation. | H·ªçc tƒÉng c∆∞·ªùng s√¢u (DRL) l√† m·ªôt k·ªπ thu·∫≠t m√°y h·ªçc ƒë∆∞·ª£c s·ª≠ d·ª•ng trong FL ƒë·ªÉ c·∫£i thi·ªán chi·∫øn l∆∞·ª£c hu·∫•n luy·ªán v√† ph√¢n b·ªï t√†i nguy√™n. |
| **Aggregation Server** | A central server that collects local model updates from clients and combines them to update the global model. | M√°y ch·ªß t·ªïng h·ª£p l√† m√°y ch·ªß trung t√¢m thu th·∫≠p c√°c b·∫£n c·∫≠p nh·∫≠t m√¥ h√¨nh t·ª´ c√°c thi·∫øt b·ªã kh√°ch v√† k·∫øt h·ª£p ch√∫ng ƒë·ªÉ c·∫≠p nh·∫≠t m√¥ h√¨nh to√†n c·ª•c. |
| **Communication Round** | A cycle in FL where local clients train models and send updates to the central server. | V√≤ng giao ti·∫øp l√† m·ªôt chu k·ª≥ trong FL, trong ƒë√≥ c√°c thi·∫øt b·ªã kh√°ch hu·∫•n luy·ªán m√¥ h√¨nh v√† g·ª≠i b·∫£n c·∫≠p nh·∫≠t l√™n m√°y ch·ªß trung t√¢m. |
| **Local Model Training** | The process where each client trains a model using its own data before sending updates to the aggregation server. | Hu·∫•n luy·ªán m√¥ h√¨nh c·ª•c b·ªô l√† qu√° tr√¨nh trong ƒë√≥ m·ªói thi·∫øt b·ªã kh√°ch t·ª± hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng d·ªØ li·ªáu c·ªßa ch√≠nh n√≥ tr∆∞·ªõc khi g·ª≠i b·∫£n c·∫≠p nh·∫≠t ƒë·∫øn m√°y ch·ªß t·ªïng h·ª£p. |
| **Model Convergence** | The state where the global model reaches an optimal accuracy after several training iterations. | H·ªôi t·ª• m√¥ h√¨nh l√† tr·∫°ng th√°i khi m√¥ h√¨nh to√†n c·ª•c ƒë·∫°t ƒë·ªô ch√≠nh x√°c t·ªëi ∆∞u sau nhi·ªÅu v√≤ng hu·∫•n luy·ªán. |
| **Non-IID Data** | Data that is not independently and identically distributed across clients, making FL training more challenging. | D·ªØ li·ªáu kh√¥ng ƒë·ªôc l·∫≠p v√† ƒë·ªìng nh·∫•t (Non-IID) l√† d·ªØ li·ªáu kh√¥ng ƒë∆∞·ª£c ph√¢n ph·ªëi ƒë·ªìng ƒë·ªÅu gi·ªØa c√°c thi·∫øt b·ªã kh√°ch, l√†m cho qu√° tr√¨nh hu·∫•n luy·ªán FL kh√≥ khƒÉn h∆°n. |
| **Heterogeneous Data** | Data collected from different sources with varying distributions and formats. | D·ªØ li·ªáu kh√¥ng ƒë·ªìng nh·∫•t l√† d·ªØ li·ªáu ƒë∆∞·ª£c thu th·∫≠p t·ª´ c√°c ngu·ªìn kh√°c nhau v·ªõi s·ª± ph√¢n b·ªë v√† ƒë·ªãnh d·∫°ng kh√°c nhau. |
| **Model Poisoning Attack** | A security attack where malicious clients send manipulated updates to corrupt the global model. | T·∫•n c√¥ng l√†m nhi·ªÖm ƒë·ªôc m√¥ h√¨nh l√† m·ªôt ki·ªÉu t·∫•n c√¥ng an ninh trong ƒë√≥ c√°c thi·∫øt b·ªã kh√°ch ƒë·ªôc h·∫°i g·ª≠i b·∫£n c·∫≠p nh·∫≠t b·ªã thay ƒë·ªïi ƒë·ªÉ ph√° ho·∫°i m√¥ h√¨nh to√†n c·ª•c. |
| **Inference Attack** | A privacy attack where an adversary attempts to infer sensitive information from a trained model. | T·∫•n c√¥ng suy lu·∫≠n l√† m·ªôt ki·ªÉu t·∫•n c√¥ng quy·ªÅn ri√™ng t∆∞, trong ƒë√≥ k·∫ª t·∫•n c√¥ng c·ªë g·∫Øng suy ra th√¥ng tin nh·∫°y c·∫£m t·ª´ m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán. |
| **Secure Multi-Party Computation (SMPC)** | A cryptographic technique allowing multiple parties to jointly compute a function while keeping inputs private. | T√≠nh to√°n ƒëa b√™n an to√†n (SMPC) l√† m·ªôt k·ªπ thu·∫≠t m√£ h√≥a cho ph√©p nhi·ªÅu b√™n c√πng t√≠nh to√°n m·ªôt h√†m m√† kh√¥ng c·∫ßn ti·∫øt l·ªô d·ªØ li·ªáu ƒë·∫ßu v√†o. |
| **Homomorphic Encryption** | An encryption method that allows computation on encrypted data without decryption. | M√£ h√≥a ƒë·ªìng h√¨nh l√† m·ªôt ph∆∞∆°ng ph√°p m√£ h√≥a cho ph√©p th·ª±c hi·ªán t√≠nh to√°n tr√™n d·ªØ li·ªáu ƒë√£ m√£ h√≥a m√† kh√¥ng c·∫ßn gi·∫£i m√£. |
| **Blockchain for FL** | Using blockchain to decentralize FL, ensuring trust, security, and transparency in the learning process. | Blockchain trong FL l√† vi·ªác s·ª≠ d·ª•ng blockchain ƒë·ªÉ ph√¢n c·∫•p FL, ƒë·∫£m b·∫£o t√≠nh tin c·∫≠y, b·∫£o m·∫≠t v√† minh b·∫°ch trong qu√° tr√¨nh h·ªçc m√°y. |
| **Edge Computing** | A computing paradigm that processes data closer to its source rather than relying on a centralized server. | ƒêi·ªán to√°n bi√™n l√† m√¥ h√¨nh x·ª≠ l√Ω d·ªØ li·ªáu g·∫ßn v·ªõi ngu·ªìn d·ªØ li·ªáu thay v√¨ ph·ª• thu·ªôc v√†o m√°y ch·ªß t·∫≠p trung. |
| **Overfitting in FL** | When a local model becomes too specialized in client data, leading to poor generalization in the global model. | Qu√° kh·ªõp trong FL x·∫£y ra khi m√¥ h√¨nh c·ª•c b·ªô tr·ªü n√™n qu√° chuy√™n bi·ªát v·ªõi d·ªØ li·ªáu c·ªßa thi·∫øt b·ªã kh√°ch, d·∫´n ƒë·∫øn kh·∫£ nƒÉng t·ªïng qu√°t k√©m trong m√¥ h√¨nh to√†n c·ª•c. |
| **Bandwidth-Efficient FL** | An FL approach that minimizes communication overhead by reducing data transmission size. | H·ªçc li√™n k·∫øt ti·∫øt ki·ªám bƒÉng th√¥ng l√† ph∆∞∆°ng ph√°p FL gi√∫p gi·∫£m t·∫£i truy·ªÅn th√¥ng b·∫±ng c√°ch gi·∫£m k√≠ch th∆∞·ªõc d·ªØ li·ªáu truy·ªÅn t·∫£i. |
| **Gradient Compression** | A technique in FL that reduces the size of model updates sent to the central server to save bandwidth. | N√©n gradient l√† m·ªôt k·ªπ thu·∫≠t trong FL gi√∫p gi·∫£m k√≠ch th∆∞·ªõc c·ªßa c√°c b·∫£n c·∫≠p nh·∫≠t m√¥ h√¨nh g·ª≠i ƒë·∫øn m√°y ch·ªß trung t√¢m ƒë·ªÉ ti·∫øt ki·ªám bƒÉng th√¥ng. |
| **Model Distillation** | A method to transfer knowledge from a larger model to a smaller one in FL. | Ch∆∞ng c·∫•t m√¥ h√¨nh l√† ph∆∞∆°ng ph√°p truy·ªÅn t·∫£i ki·∫øn th·ª©c t·ª´ m·ªôt m√¥ h√¨nh l·ªõn sang m·ªôt m√¥ h√¨nh nh·ªè h∆°n trong FL. |
| **Federated Hyperparameter Tuning** | The process of optimizing hyperparameters in FL without sharing raw data. | ƒêi·ªÅu ch·ªânh si√™u tham s·ªë li√™n k·∫øt l√† qu√° tr√¨nh t·ªëi ∆∞u h√≥a si√™u tham s·ªë trong FL m√† kh√¥ng c·∫ßn chia s·∫ª d·ªØ li·ªáu g·ªëc. |
